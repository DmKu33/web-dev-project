<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>D3.js ML Visualizations</title>
    <link rel="stylesheet" href="assets/css/styles.css">
    <script src="https://d3js.org/d3.v7.min.js"></script>
</head>

<body>
    <header>
        <h1>Machine Learning Training Dynamics</h1>
        <p>Five interactive visualizations exploring optimization, generalization, and model behavior</p>
    </header>

    <!-- DEMO 1: DECISION BOUNDARIES -->
    <section class="demo-section">
        <div class="demo-text">
            <h2>Decision Boundaries</h2>
            <p>Neural networks learn to separate classes by finding decision boundaries. Watch the boundary evolve from a poor initial guess to a circular shape that cleanly separates the inner and outer clusters.</p>
            
            <div class="controls" id="boundary-controls">
                <div>
                    <label for="epoch-slider">Epoch: <span id="epoch-slider-value">0</span></label>
                    <input type="range" id="epoch-slider" min="0" max="50" step="1" value="0">
                </div>
            </div>
            
            <button class="expand-btn">
                <span class="icon">▼</span> Detailed Explanation
            </button>
            <div class="detailed-content">
                <p>The model learns a circular boundary that separates two concentric clusters. Early epochs show a small elliptical boundary. Later epochs reveal the circular shape needed to fit the data. Neural networks with hidden layers can learn circular and other complex decision boundaries by learning nonlinear transformations of the input features.</p>
            </div>
        </div>
        
        <div class="demo-viz">
            <div id="boundary-viz"></div>
        </div>
    </section>

    <!-- DEMO 2: LOSS CURVES -->
    <section class="demo-section scroll-section">
        <div class="demo-text sticky-text">
            <h2>Loss Curves and Overfitting</h2>
            <p>Training loss keeps dropping, but validation loss starts climbing back up. This is overfitting. The model memorizes training data instead of learning patterns. Scroll to watch it happen.</p>
            
            <div class="scroll-progress">Scroll to progress through 100 epochs</div>
            
            <button class="expand-btn">
                <span class="icon">▼</span> Detailed Explanation
            </button>
            <div class="detailed-content">
                <p>Around epoch 40, validation loss stops improving and starts increasing while training loss continues to drop. This gap signals overfitting. The model becomes too specialized to training data quirks and noise, losing its ability to generalize to new examples. Solutions include early stopping (halt training when validation loss increases), regularization, dropout, or gathering more training data. The shaded regions show variance across different training runs.</p>
            </div>
        </div>
        
        <div class="demo-viz sticky-viz">
            <div id="loss-viz"></div>
        </div>
        
        <!-- spacer for scroll effect -->
        <div class="scroll-spacer"></div>
    </section>

    <!-- DEMO 3: OPTIMIZER COMPARISON -->
    <section class="demo-section">
        <div class="demo-text">
            <h2>Optimizer Comparison</h2>
            <p>Different optimizers take different paths down the same loss surface. Watch SGD and Adam navigate a 2D landscape. Each has unique strengths and trade-offs.</p>
            
            <div class="controls">
                <div>
                    <button id="reset-optimizers">Reset</button>
                    <button id="play-optimizers">Play</button>
                    <button id="step-optimizers">Step</button>
                </div>
            </div>
            
            <button class="expand-btn">
                <span class="icon">▼</span> Detailed Explanation
            </button>
            <div class="detailed-content">
                <p>SGD (Stochastic Gradient Descent) takes direct steps opposite to the gradient. Simple and reliable, but can be slow in flat regions and gets stuck in valleys. Adam is an adaptive optimizer that adjusts the learning rate per parameter using both momentum and second-moment estimates. It converges faster and handles sparse gradients well, making it the default choice for many deep learning tasks. There is no universally best optimizer; the choice depends on your problem, data characteristics, and computational constraints.</p>
            </div>
        </div>
        
        <div class="demo-viz">
            <div id="optimizer-viz"></div>
        </div>
    </section>

    <!-- DEMO 4: BIAS-VARIANCE TRADEOFF -->
    <section class="demo-section bias-variance-section">
        <div class="demo-text">
            <h2>Bias-Variance Tradeoff</h2>
            <p>Model complexity determines whether you underfit, overfit, or find the sweet spot. These four scenarios show the classic tradeoff in action with polynomial regression.</p>
            
            <button class="expand-btn">
                <span class="icon">▼</span> Detailed Explanation
            </button>
            <div class="detailed-content">
                <p>Top-left shows just-right model complexity. Fits training data well and generalizes to test data. Top-right is overfit (too complex). Perfect on training data but terrible on test data due to learning noise. Bottom-left is underfit (too simple). Poor fit on both training and test data due to inability to capture the pattern. Bottom-right is worst case (wrong model plus noisy data). Erratic predictions, unstable across datasets.</p>
            </div>
        </div>
        
        <div class="demo-viz">
            <div id="bias-variance-viz"></div>
        </div>
    </section>

    <!-- DEMO 5: FEATURE SPACE EVOLUTION -->
    <section class="demo-section">
        <div class="demo-text">
            <h2>Feature Space Evolution</h2>
            <p>Watch how a neural network learns to separate classes in feature space. As training progresses, initially mixed clusters gradually separate into distinct groups.</p>
            
            <div class="controls">
                <div>
                    <label for="feature-epoch-slider">Epoch: <span id="feature-epoch-value">0</span></label>
                    <input type="range" id="feature-epoch-slider" min="0" max="30" step="1" value="0">
                </div>
            </div>
            
            <button class="expand-btn">
                <span class="icon">▼</span> Detailed Explanation
            </button>
            <div class="detailed-content">
                <p>This shows a 2D projection of the network's internal representations. Each point is a data sample, colored by its true class. Early epochs show random and mixed features. The network has not learned meaningful representations yet. Later epochs show classes separating into distinct clusters as the network learns discriminative features. Good feature representations are the foundation of classification. This visualization shows the network learning to see the differences between classes.</p>
            </div>
        </div>
        
        <div class="demo-viz">
            <div id="feature-viz"></div>
        </div>
    </section>

    <!-- ABOUT & REFERENCES -->
    <section class="references-section">
        <div class="references-container">
            <h2>About This Project</h2>
            <div class="about-text">
                <p>This project explores machine learning concepts through interactive visualizations. I selected topics that are inherently visual, including decision boundaries, loss landscapes, optimizer behavior, bias-variance tradeoffs, and feature space evolution, because animation and interactivity make these concepts more intuitive than static explanations.</p>
                <p>Built with D3.js v7, the project experiments with natural animations and scroll-driven storytelling, inspired by sites like Distill.pub and r2d3.us. All data is synthetically generated in JavaScript using mathematical functions and procedural generation.</p>
            </div>
            
            <h2 style="margin-top: 3rem;">Tools & Further Reading</h2>
            <div class="references-grid">
                <div class="reference-item">
                    <h3>Libraries Used</h3>
                    <ul>
                        <li>D3.js v7 for interactive visualizations</li>
                        <li>Scikit-learn for exploring dataset patterns during development</li>
                    </ul>
                </div>
                <div class="reference-item">
                    <h3>Further Reading</h3>
                    <ul>
                        <li>Ruder, S. (2016). An overview of gradient descent optimization algorithms. <em>arXiv:1609.04747</em></li>
                        <li>Goodfellow, Bengio & Courville (2016). <em>Deep Learning</em>. MIT Press</li>
                    </ul>
                </div>
                <div class="reference-item">
                    <h3>Inspiration</h3>
                    <ul>
                        <li><a href="https://distill.pub" target="_blank" style="color: #b0b0b0;">Distill.pub</a> - Visual explanations of ML</li>
                        <li><a href="https://r2d3.us" target="_blank" style="color: #b0b0b0;">r2d3.us</a> - Scroll-driven data stories</li>
                    </ul>
                </div>
            </div>
        </div>
    </section>

    <footer>
        <p>Interactive ML Visualizations · Built with D3.js</p>
    </footer>

    <script src="assets/js/main.js"></script>
</body>
</html>
